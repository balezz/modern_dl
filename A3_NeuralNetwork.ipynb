{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "A3-NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHTx7qtNqTmr"
      },
      "source": [
        "# Задание 3 - Нейронные сети\n",
        "\n",
        "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
        "\n",
        "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o0LkWMusLbT"
      },
      "source": [
        "# Load data\n",
        "%%bash\n",
        "git clone https://github.com/balezz/modern_dl.git\n",
        "cd modern_dl\n",
        "mkdir data\n",
        "cd data\n",
        "wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
        "tar -xzvf cifar-10-python.tar.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpufynl_rSC-"
      },
      "source": [
        "%cd modern_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iDDqkCVqTm1"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from lib.data_utils import load_CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vksu9_hshtH"
      },
      "source": [
        "# Путь к папке с данными\n",
        "cifar10_dir = 'data/cifar-10-batches-py'\n",
        "\n",
        "# Очистим значения переменных, чтобы избежать проблем с излишним потреблением памяти\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbbr8Lsgshpf"
      },
      "source": [
        "# Перед началом работы полезно посмотреть на данные.\n",
        "# Отобразим пример из каждого класса.\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(X_train[idx].astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV4zBk0-s3Y0"
      },
      "source": [
        "# Разделим данные на выборки train, val, test\n",
        "\n",
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "num_dev = 16\n",
        "\n",
        "# Для валидации используем подвыборку train\n",
        "mask = range(num_training, num_training + num_validation)\n",
        "X_val = X_train[mask]\n",
        "y_val = y_train[mask]\n",
        "\n",
        "# На остальных данных из train будем тренировать модель\n",
        "mask = range(num_training)\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "# Чтобы ускорить разработку, создадим также небольшую dev выборку \n",
        "mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "X_dev = X_train[mask]\n",
        "y_dev = y_train[mask]\n",
        "\n",
        "# Для тестирования используем оригинальную выборку test\n",
        "mask = range(num_test)\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj0qwiwLswdI"
      },
      "source": [
        "# Преобразуем двумерные изображения в одномерные вектора\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E11G6K95swaT"
      },
      "source": [
        "# Нормализуем значения яркости пикселей \n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) \n",
        "\n",
        "# визуализируем среднюю яркость\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
        "plt.show()\n",
        "\n",
        "# Вычтем средние значения яркости\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "X_dev -= mean_image\n",
        "\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZYwEb83qTm2"
      },
      "source": [
        "from lib.gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RENeyXUWqTm8"
      },
      "source": [
        "# Как всегда, начинаем с кирпичиков\n",
        "\n",
        "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
        "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
        "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
        "\n",
        "Начнем с ReLU, у которого параметров нет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgCITVZ_vqqG"
      },
      "source": [
        "class ReLULayer:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: Implement forward pass\n",
        "        # Hint: you'll need to save some information about X\n",
        "        # to use it later in the backward pass\n",
        "        #raise Exception(\"Not implemented!\")\n",
        "        self.mask = X > 0\n",
        "        return X * self.mask\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Arguments:\n",
        "        d_out, np array (batch_size, num_features) - gradient\n",
        "           of loss function with respect to output\n",
        "        Returns:\n",
        "        d_result: np array (batch_size, num_features) - gradient\n",
        "          with respect to input\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        # Your final implementation shouldn't have any loops\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        return d_out * self.mask\n",
        "\n",
        "    def params(self):\n",
        "        # ReLU Doesn't have any parameters\n",
        "        return {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc5nbuadqTm-"
      },
      "source": [
        "# TODO: Implement ReLULayer \n",
        "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
        "\n",
        "X = np.array([[1,-2,3],\n",
        "              [-1, 2, 0.1]\n",
        "              ])\n",
        "\n",
        "assert check_layer_gradient(ReLULayer(), X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFORmUG0qTm-"
      },
      "source": [
        "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
        "\n",
        "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
        "\n",
        "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwKPVms4wooe"
      },
      "source": [
        "class Param:\n",
        "    \"\"\"\n",
        "    Trainable parameter of the model\n",
        "    Captures both parameter value and the gradient\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.grad = np.zeros_like(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toNth1PKwbos"
      },
      "source": [
        "class FullyConnectedLayer:\n",
        "    def __init__(self, n_input, n_output):\n",
        "        self.W = Param(0.001 * np.random.randn(n_input, n_output))\n",
        "        self.B = Param(0.001 * np.random.randn(1, n_output))\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: Implement forward pass\n",
        "        # Your final implementation shouldn't have any loops\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        self.X = X\n",
        "        return X @ self.W.value + self.B.value\n",
        "\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Computes gradient with respect to input and\n",
        "        accumulates gradients within self.W and self.B\n",
        "        Arguments:\n",
        "        d_out, np array (batch_size, n_output) - gradient\n",
        "           of loss function with respect to output\n",
        "        Returns:\n",
        "        d_result: np array (batch_size, n_input) - gradient\n",
        "          with respect to input\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        # Compute both gradient with respect to input\n",
        "        # and gradients with respect to W and B\n",
        "        # Add gradients of W and B to their `grad` attribute\n",
        "\n",
        "        # It should be pretty similar to linear classifier from\n",
        "        # the previous assignment\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        # print('d_out shape is ', d_out.shape)\n",
        "        # print('self.W shape is ', self.W.value.shape)\n",
        "        d_input = d_out @ self.W.value.T\n",
        "        self.W.grad = self.X.T @ d_out \n",
        "        self.B.grad = np.sum(d_out, axis=0).reshape(1, -1)\n",
        "        # print(self.B.grad.shape)\n",
        "\n",
        "        return d_input\n",
        "\n",
        "    def params(self):\n",
        "        return {'W': self.W, 'B': self.B}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kI8wlgoCqTm_"
      },
      "source": [
        "# TODO: Implement FullyConnected layer forward and backward methods\n",
        "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
        "# TODO: Implement storing gradients for W and B\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJW7H6pqTnA"
      },
      "source": [
        "## Создаем нейронную сеть\n",
        "\n",
        "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
        "\n",
        "Не забудьте реализовать очистку градиентов в начале функции."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_UAbBDaB9I2"
      },
      "source": [
        "# Copied from previous\n",
        "def softmax_with_cross_entropy(X, y):\n",
        "    \"\"\"\n",
        "    Computes softmax and cross-entropy loss for model predictions,\n",
        "    including the gradient\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
        "        classifier output\n",
        "      target_index: np array of int, shape is (1) or (batch_size) -\n",
        "        index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
        "    \"\"\"\n",
        "    N = X.shape[0]\n",
        "    X -= np.max(X)\n",
        "    Exps = np.exp(X)\n",
        "    # S.shape = N, C\n",
        "    S = Exps / Exps.sum(axis=1, keepdims=True)\n",
        "    # loss.shape = 1\n",
        "    loss = - np.log(S[range(N), y]).mean()\n",
        "    S[range(N), y] -= 1\n",
        "    d_out = S / N\n",
        "    return loss, d_out\n",
        "\n",
        "def l2_regularization(W, reg_strength):\n",
        "    loss = 0.5 * reg_strength * np.sum(W*W)\n",
        "    grad = np.dot(W,reg_strength)\n",
        "    return loss, grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJi2DaKFw3ey"
      },
      "source": [
        "class TwoLayerNet:\n",
        "    \"\"\" Neural network with two fully connected layers \"\"\"\n",
        "\n",
        "    def __init__(self, n_input, n_output, hidden_layer_size, reg):\n",
        "        \"\"\"\n",
        "        Initializes the neural network\n",
        "        Arguments:\n",
        "        n_input, int - dimension of the model input\n",
        "        n_output, int - number of classes to predict\n",
        "        hidden_layer_size, int - number of neurons in the hidden layer\n",
        "        reg, float - L2 regularization strength\n",
        "        \"\"\"\n",
        "        self.reg = reg\n",
        "        # TODO Create necessary layers\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        self.layers = [FullyConnectedLayer(n_input, hidden_layer_size), \n",
        "                       ReLULayer(), \n",
        "                       FullyConnectedLayer(hidden_layer_size, n_output),\n",
        "                       ReLULayer()]\n",
        "\n",
        "\n",
        "    def compute_loss_and_gradients(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes total loss and updates parameter gradients\n",
        "        on a batch of training examples\n",
        "        Arguments:\n",
        "        X, np array (batch_size, input_features) - input data\n",
        "        y, np array of int (batch_size) - classes\n",
        "        \"\"\"\n",
        "        \n",
        "        # Before running forward and backward pass through the model,\n",
        "        # clear parameter gradients aggregated from the previous pass\n",
        "        # TODO Set parameter gradient to zeros\n",
        "        # Hint: using self.params() might be useful!\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        X = X.copy()\n",
        "        for param in self.params().values():\n",
        "          param.grad = np.zeros_like(param.grad)\n",
        "\n",
        "        # TODO Compute loss and fill param gradients\n",
        "        # by running forward and backward passes through the model\n",
        "        for layer in self.layers:\n",
        "          X = layer.forward(X)\n",
        "        \n",
        "        # After that, implement l2 regularization on all params\n",
        "        # Hint: self.params() is useful again!\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "\n",
        "        loss, d_out = softmax_with_cross_entropy(X, y)\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "          d_out = layer.backward(d_out)\n",
        "          reg_grad = 0\n",
        "          for param in layer.params().values():\n",
        "            reg_loss, reg_grad = l2_regularization(param.value, self.reg)\n",
        "            param.grad += reg_grad\n",
        "            loss += reg_loss \n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Produces classifier predictions on the set\n",
        "        Arguments:\n",
        "          X, np array (test_samples, num_features)\n",
        "        Returns:\n",
        "          y_pred, np.array of int (test_samples)\n",
        "        \"\"\"\n",
        "        # TODO: Implement predict\n",
        "        # Hint: some of the code of the compute_loss_and_gradients\n",
        "        # can be reused\n",
        "        pred = np.zeros(X.shape[0], np.int)\n",
        "        for layer in self.layers:\n",
        "          X = layer.forward(X)\n",
        "        pred = np.argmax(X, axis=1)\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        return pred\n",
        "\n",
        "    def params(self):\n",
        "        result = {}\n",
        "        name = 0\n",
        "        # TODO Implement aggregating all of the params\n",
        "        for layer in self.layers:\n",
        "          params = layer.params()\n",
        "          for p in params.values():\n",
        "            result[f'p{name}'] = p\n",
        "            name += 1\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLg4aUyAqTnA"
      },
      "source": [
        "# TODO: implement compute_loss_and_gradients function\n",
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
        "loss = model.compute_loss_and_gradients(X_train[:2], y_train[:2])\n",
        "\n",
        "# TODO Now implement backward pass and aggregate all of the params\n",
        "check_model_gradient(model, X_train[:2], y_train[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i17Dtu83qTnB"
      },
      "source": [
        "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dMNZMrPqTnC"
      },
      "source": [
        "# TODO Now implement l2 regularization in the forward and backward pass\n",
        "model_with_reg = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
        "loss_with_reg = model_with_reg.compute_loss_and_gradients(X_train[:2], y_train[:2])\n",
        "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
        "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
        "\n",
        "check_model_gradient(model_with_reg, X_train[:2], y_train[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OhlbDyIqTnD"
      },
      "source": [
        "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
        "\n",
        "Какое значение точности мы ожидаем увидеть до начала тренировки?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvgmwuEWyEuj"
      },
      "source": [
        "def multiclass_accuracy(y_true, y_pred):\n",
        "  return np.mean(np.equal(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BjF-QEgqTnD"
      },
      "source": [
        "# Finally, implement predict function!\n",
        "\n",
        "# TODO: Implement predict function\n",
        "# What would be the value we expect?\n",
        "multiclass_accuracy(model_with_reg.predict(X_train[:30]), y_train[:30]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRjzawUJqTnD"
      },
      "source": [
        "# Допишем код для процесса тренировки\n",
        "\n",
        "Если все реализовано корректно, значение функции ошибки должно уменьшаться в некоторых эпохах. Не беспокойтесь пока про validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYxUpX2Q5bTA"
      },
      "source": [
        "from copy import deepcopy\n",
        "from lib.data_utils import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXjIXATS3Tuc"
      },
      "source": [
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Trainer of the neural network models\n",
        "    Perform mini-batch SGD with the specified data, model,\n",
        "    training parameters and optimization rule\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, dataset, optim,\n",
        "                 num_epochs=20,\n",
        "                 batch_size=20,\n",
        "                 learning_rate=1e-2,\n",
        "                 learning_rate_decay=1.0):\n",
        "        \"\"\"\n",
        "        Initializes the trainer\n",
        "        Arguments:\n",
        "        model - neural network model\n",
        "        dataset, instance of Dataset class - data to train on\n",
        "        optim - optimization method (see optim.py)\n",
        "        num_epochs, int - number of epochs to train\n",
        "        batch_size, int - batch size\n",
        "        learning_rate, float - initial learning rate\n",
        "        learning_rate_decal, float - ratio for decaying learning rate\n",
        "           every epoch\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "\n",
        "        self.optimizers = None\n",
        "\n",
        "    def setup_optimizers(self):\n",
        "        params = self.model.params()\n",
        "        self.optimizers = {}\n",
        "        for param_name, param in params.items():\n",
        "            self.optimizers[param_name] = deepcopy(self.optim)\n",
        "\n",
        "    def compute_accuracy(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes accuracy on provided data using mini-batches\n",
        "        \"\"\"\n",
        "        indices = np.arange(X.shape[0])\n",
        "        sections = np.arange(self.batch_size, X.shape[0], self.batch_size)\n",
        "        batches_indices = np.array_split(indices, sections)\n",
        "\n",
        "        pred = np.zeros_like(y)\n",
        "\n",
        "        for batch_indices in batches_indices:\n",
        "            batch_X = X[batch_indices]\n",
        "            pred_batch = self.model.predict(batch_X)\n",
        "            pred[batch_indices] = pred_batch\n",
        "\n",
        "        return multiclass_accuracy(pred, y)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Trains a model\n",
        "        \"\"\"\n",
        "        if self.optimizers is None:\n",
        "            self.setup_optimizers()\n",
        "\n",
        "        num_train = self.dataset.train_X.shape[0]\n",
        "\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            shuffled_indices = np.arange(num_train)\n",
        "            np.random.shuffle(shuffled_indices)\n",
        "            sections = np.arange(self.batch_size, num_train, self.batch_size)\n",
        "            batches_indices = np.array_split(shuffled_indices, sections)\n",
        "\n",
        "            batch_losses = []\n",
        "\n",
        "            for batch_indices in batches_indices:\n",
        "                # TODO Generate batches based on batch_indices and\n",
        "                # use model to generate loss and gradients for all\n",
        "                # the params\n",
        "\n",
        "                # raise Exception(\"Not implemented!\")\n",
        "                X_batch = self.dataset.train_X[batch_indices]\n",
        "                y_batch = self.dataset.train_y[batch_indices]\n",
        "                loss = self.model.compute_loss_and_gradients(X_batch, y_batch)\n",
        "\n",
        "                for param_name, param in self.model.params().items():\n",
        "                    optimizer = self.optimizers[param_name]\n",
        "                    param.value = optimizer.update(param.value, param.grad, self.learning_rate)\n",
        "\n",
        "                batch_losses.append(loss)\n",
        "\n",
        "            if np.not_equal(self.learning_rate_decay, 1.0):\n",
        "                # TODO: Implement learning rate decay\n",
        "                self.learning_rate *= self.learning_rate_decay \n",
        "\n",
        "            ave_loss = np.mean(batch_losses)\n",
        "\n",
        "            train_accuracy = self.compute_accuracy(self.dataset.train_X,\n",
        "                                                   self.dataset.train_y)\n",
        "\n",
        "            val_accuracy = self.compute_accuracy(self.dataset.val_X,\n",
        "                                                 self.dataset.val_y)\n",
        "\n",
        "            print(\"Loss: %f, Train accuracy: %f, val accuracy: %f\" %\n",
        "                  (batch_losses[-1], train_accuracy, val_accuracy))\n",
        "\n",
        "            loss_history.append(ave_loss)\n",
        "            train_acc_history.append(train_accuracy)\n",
        "            val_acc_history.append(val_accuracy)\n",
        "\n",
        "        return loss_history, train_acc_history, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI7UwbYYHGQD"
      },
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Implements vanilla SGD update\n",
        "    \"\"\"\n",
        "    def update(self, w, d_w, learning_rate):\n",
        "        \"\"\"\n",
        "        Performs SGD update\n",
        "        Arguments:\n",
        "        w, np array - weights\n",
        "        d_w, np array, same shape as w - gradient\n",
        "        learning_rate, float - learning rate\n",
        "        Returns:\n",
        "        updated_weights, np array same shape as w\n",
        "        \"\"\"\n",
        "        return w - d_w * learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu2GMEUxqTnE"
      },
      "source": [
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_dev, y_dev)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-4)\n",
        "\n",
        "# TODO Implement missing pieces in Trainer.fit function\n",
        "# You should expect loss to go down every epoch, even if it's slow\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMmsiUG4qTnF"
      },
      "source": [
        "# Улучшаем процесс тренировки\n",
        "\n",
        "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "360qTMryqTnF"
      },
      "source": [
        "## Уменьшение скорости обучения (learning rate decay)\n",
        "\n",
        "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
        "\n",
        "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
        "\n",
        "В нашем случае N будет равным 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdYzqWT7qTnG"
      },
      "source": [
        "# TODO Implement learning rate decay inside Trainer.fit method\n",
        "# Decay should happen once per epoch\n",
        "\n",
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_val, y_val)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-4, learning_rate_decay=0.99)\n",
        "\n",
        "initial_learning_rate = trainer.learning_rate\n",
        "loss_history, train_history, val_history = trainer.fit()\n",
        "\n",
        "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
        "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thY1Jmf-qTnH"
      },
      "source": [
        "# Накопление импульса (Momentum SGD)\n",
        "\n",
        "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
        "\n",
        "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
        "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
        "\n",
        "```\n",
        "velocity = momentum * velocity - learning_rate * gradient \n",
        "w = w + velocity\n",
        "```\n",
        "\n",
        "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
        "\n",
        "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
        "http://cs231n.github.io/neural-networks-3/#sgd  \n",
        "https://distill.pub/2017/momentum/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGpzlGOi6d1j"
      },
      "source": [
        "class MomentumSGD:\n",
        "    \"\"\"\n",
        "    Implements Momentum SGD update\n",
        "    \"\"\"\n",
        "    def __init__(self, momentum=0.9):\n",
        "        self.momentum = 0.9\n",
        "        self.velocity = 0\n",
        "    \n",
        "    def update(self, w, d_w, learning_rate):\n",
        "        \"\"\"\n",
        "        Performs Momentum SGD update\n",
        "        Arguments:\n",
        "        w, np array - weights\n",
        "        d_w, np array, same shape as w - gradient\n",
        "        learning_rate, float - learning rate\n",
        "        Returns:\n",
        "        updated_weights, np array same shape as w\n",
        "        \"\"\"\n",
        "        # TODO Implement momentum update\n",
        "        # Hint: you'll need to introduce some variables to remember\n",
        "        # velocity from the previous updates\n",
        "        # raise Exception(\"Not implemented!\")\n",
        "        self.velocity = self.momentum * self.velocity - d_w * learning_rate\n",
        "\n",
        "        return w + self.velocity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7XF4uoeqTnI"
      },
      "source": [
        "# TODO: Implement MomentumSGD.update function \n",
        "\n",
        "model = TwoLayerNet(n_input = X_dev.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_val, y_val)\n",
        "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
        "\n",
        "# You should see even better results than before!\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VW6895HqTnI"
      },
      "source": [
        "# Ну что, давайте уже тренировать сеть!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_-F6WL1qTnI"
      },
      "source": [
        "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
        "\n",
        "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
        "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
        "\n",
        "Если этого не происходит, то где-то была допущена ошибка!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzdNQcUqTnJ"
      },
      "source": [
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 64, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_val, y_val)\n",
        "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, num_epochs=150, batch_size=8)\n",
        "\n",
        "# You should expect this to reach 1.0 training accuracy \n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM6p4N7PqTnL"
      },
      "source": [
        "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
        "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
        "Найдите их!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF5q9UpmqTnL"
      },
      "source": [
        "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
        "\n",
        "model = TwoLayerNet(n_input = X_train.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(X_dev, y_dev, X_val, y_val)\n",
        "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
        "\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvbgvvqpqTnM"
      },
      "source": [
        "# Итак, основное мероприятие!\n",
        "\n",
        "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
        "\n",
        "Добейтесь точности лучше **40%** на validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3svNLCiuqTnN"
      },
      "source": [
        "# Let's train the best one-hidden-layer network we can\n",
        "\n",
        "learning_rate = 1e-4\n",
        "reg_strength = 1e-3\n",
        "learning_rate_decay = 0.999\n",
        "hidden_layer_size = 128\n",
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "\n",
        "best_classifier = None\n",
        "best_val_accuracy = None\n",
        "\n",
        "loss_history = []\n",
        "train_history = []\n",
        "val_history = []\n",
        "\n",
        "# TODO find the best hyperparameters to train the network\n",
        "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
        "# You should expect to get to at least 40% of valudation accuracy\n",
        "# Save loss/train/history of the best classifier to the variables above\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T6Xm6PxqTnN"
      },
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.subplot(211)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(loss_history)\n",
        "plt.subplot(212)\n",
        "plt.title(\"Train/validation accuracy\")\n",
        "plt.plot(train_history)\n",
        "plt.plot(val_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwlrmE1TqTnN"
      },
      "source": [
        "# Посмотрим, как наша лучшая модель работает на тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKw5JN6nqTnN"
      },
      "source": [
        "test_pred = best_classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}